# v10 RL Hybrid ì „ëµ ê²°ê³¼ ë³´ê³ ì„œ

## ğŸ“Š Executive Summary

**v10 ì „ëµ**: Reinforcement Learning (PPO) ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
**ëª©í‘œ**: v5 (293.38%) ì´ˆê³¼, RLë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ
**ê²°ê³¼**: **ì™„ì „ ì‹¤íŒ¨** - 0% ìˆ˜ìµë¥ , ê±°ë˜ 0íšŒ

## âŒ ìµœì¢… ì„±ê³¼ (2025-10-19)

### Train Set (2018-09-04 ~ 2023-12-31)
```yaml
ìˆ˜ìµë¥ : 0.00%
ê±°ë˜: 0íšŒ
ìŠ¹ë¥ : N/A
Action ë¶„í¬:
  - Buy: 0íšŒ (0.0%)
  - Sell: 1,880íšŒ (100.0%)
  - Hold: 0íšŒ (0.0%)
```

### Validation 2024 (2024-01-01 ~ 2024-12-30)
```yaml
ìˆ˜ìµë¥ : 0.00%
ê±°ë˜: 0íšŒ
ìŠ¹ë¥ : N/A
vs v5: -293.38%p
vs Buy&Hold (137.49%): -137.49%p
Action ë¶„í¬:
  - Buy: 0íšŒ (0.0%)
  - Sell: 300íšŒ (100.0%)
  - Hold: 0íšŒ (0.0%)
```

### Test 2025 (2025-01-01 ~ 2025-10-17)
```yaml
ìˆ˜ìµë¥ : 0.00%
ê±°ë˜: 0íšŒ
ìŠ¹ë¥ : N/A
vs v5: -124.93%p
vs Buy&Hold (20.15%): -20.15%p
Action ë¶„í¬:
  - Buy: 0íšŒ (0.0%)
  - Sell: 225íšŒ (100.0%)
  - Hold: 0íšŒ (0.0%)
```

## ğŸ¯ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€

| ëª©í‘œ | ë‹¬ì„± ì—¬ë¶€ | ì‹¤ì œ ê²°ê³¼ |
|------|-----------|-----------|
| 2024 ìˆ˜ìµë¥  150-180% | âŒ | 0.00% (-150.00%p) |
| 2025 ìˆ˜ìµë¥  40-60% | âŒ | 0.00% (-40.00%p) |
| ì˜¤ë²„í”¼íŒ… <50% | N/A | ì¸¡ì • ë¶ˆê°€ (ìˆ˜ìµ ì—†ìŒ) |
| v5 (293.38%) ì´ˆê³¼ | âŒ | 0.00% (-293.38%p) |

## ğŸ” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„

### 1. Reward Function ì„¤ê³„ ì˜¤ë¥˜

**ë¬¸ì œ**: Agentê°€ "ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠëŠ” ê²ƒ"ì„ í•™ìŠµ

```python
# í˜„ì¬ Reward êµ¬ì¡°
- í¬ì§€ì…˜ ì—†ìŒ + ì•¡ì…˜ ì—†ìŒ: Reward 0
- ë§¤ìˆ˜/ë§¤ë„ ì‹œ: Reward -0.05% (ìˆ˜ìˆ˜ë£Œ íŒ¨ë„í‹°)
- ì´ìµ ì‹¤í˜„ ì‹œ: Reward +profit_pct
```

**Agentì˜ í•™ìŠµ ê²°ê³¼**:
- "ê±°ë˜í•˜ë©´ ìˆ˜ìˆ˜ë£Œ ì†ì‹¤ â†’ ê±°ë˜ ì•ˆ í•˜ë©´ ì†ì‹¤ ì—†ìŒ"
- "ë§¤ìˆ˜í•˜ì§€ ì•Šìœ¼ë©´ ì†ì‹¤ ìœ„í—˜ë„ ì—†ìŒ"
- **ê²°ë¡ **: ë§¤ë„ ì•¡ì…˜ë§Œ ë°˜ë³µ (í¬ì§€ì…˜ ì—†ëŠ” ìƒíƒœì—ì„œ ë§¤ë„ëŠ” ë¬´íš¨ â†’ Reward 0 ìœ ì§€)

### 2. State Spaceì˜ ì´ˆê¸°ê°’ ë¬¸ì œ

**ê´€ì¸¡ê°’ ì´ˆê¸° ìƒíƒœ**:
```
Position: 0 (í¬ì§€ì…˜ ì—†ìŒ)
Cash Fraction: 1.0 (í˜„ê¸ˆ 100%)
Profit: 0 (ìˆ˜ìµ ì—†ìŒ)
```

**Agent í•™ìŠµ**:
- ì´ ìƒíƒœì—ì„œ Hold â†’ Reward 0
- ì´ ìƒíƒœì—ì„œ Buy â†’ Reward -0.0005 (ìˆ˜ìˆ˜ë£Œ)
- ì´ ìƒíƒœì—ì„œ Sell (ë¬´íš¨) â†’ Reward 0

â†’ **Sell ì•¡ì…˜ì´ ê°€ì¥ ì•ˆì „** (ìˆ˜ìˆ˜ë£Œ ì—†ì´ Reward 0 ìœ ì§€)

### 3. Exploration vs Exploitation ì‹¤íŒ¨

**PPO ì„¤ì •**:
```yaml
Learning Rate: 0.0003
Gamma: 0.99
n_steps: 2048
Exploration: Entropy Coefficient 0.0
```

**ë¬¸ì œ**:
- Entropy Coefficient 0.0 â†’ íƒí—˜ ë¶€ì¡±
- ì´ˆê¸°ë¶€í„° Sell ì•¡ì…˜ ì„ í˜¸ â†’ ë‹¤ë¥¸ ì•¡ì…˜ ì‹œë„ ì•ˆ í•¨
- PPOì˜ Conservative íŠ¹ì„± â†’ ì•ˆì „í•œ ì „ëµ(Sell) ê³ ìˆ˜

### 4. Episode Length vs Timesteps ë¶ˆì¼ì¹˜

**í™˜ê²½**:
- Episode Length: 1,880 (5.3ë…„ DAY ë°ì´í„°)
- Total Timesteps: 100,000

**ì‹¤ì œ í•™ìŠµ**:
- 100,000 / 1,880 = ì•½ 53 ì—í”¼ì†Œë“œ
- ì—í”¼ì†Œë“œë‹¹ í•™ìŠµ ê¸°íšŒ ë„ˆë¬´ ì ìŒ
- Long-term Rewardë¥¼ í•™ìŠµí•˜ê¸° ë¶€ì¡±

### 5. ì•”ë¬µì  Buy&Hold ì „ëµ í•™ìŠµ ì‹¤íŒ¨

**ê¸°ëŒ€**:
- Agentê°€ "ë§¤ìˆ˜ í›„ ìƒìŠ¹ ì‹œ ë³´ìœ , í•˜ë½ ì‹œ ë§¤ë„"ë¥¼ í•™ìŠµ

**ì‹¤ì œ**:
- "ë§¤ìˆ˜" ìì²´ë¥¼ í•™ìŠµí•˜ì§€ ëª»í•¨
- Sell ì•¡ì…˜ë§Œ ë°˜ë³µ â†’ ê±°ë˜ 0íšŒ â†’ Reward 0 ê³ ì°©

## ğŸ§ª í•™ìŠµ ê³¼ì • ë¶„ì„

### Training Log ìš”ì•½

```
Iteration 1:  ep_rew_mean: -441
Iteration 10: ep_rew_mean: -514
Iteration 20: ep_rew_mean: -512
Iteration 30: ep_rew_mean: -480
Iteration 49: ep_rew_mean: -467
```

**ê´€ì°°**:
- Episode Reward í•­ìƒ ìŒìˆ˜ (-400 ~ -500)
- í•™ìŠµ ì§„í–‰í•´ë„ Reward ê°œì„  ì—†ìŒ
- Value Loss ë†’ìŒ (8-12 ë²”ìœ„ ìœ ì§€)
- Policy Loss ë§¤ìš° ë‚®ìŒ (~0.00) â†’ ì •ì±… ë³€í™” ê±°ì˜ ì—†ìŒ

### Evaluation ê²°ê³¼

```
Eval at 20k: mean_reward 0.00, episode_length 300
Eval at 40k: mean_reward 0.00, episode_length 300
Eval at 60k: mean_reward 0.00, episode_length 300
Eval at 80k: mean_reward 0.00, episode_length 300
Eval at 100k: mean_reward 0.00, episode_length 300
```

**ë¬¸ì œ**:
- ê²€ì¦ í™˜ê²½ì—ì„œ í•­ìƒ Episode Length 300ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ
- ì‹¤ì œ ë°ì´í„°ëŠ” 331ê°œì¸ë° 300ì—ì„œ ë©ˆì¶¤
- Agentê°€ í™˜ê²½ì„ ì œëŒ€ë¡œ íƒìƒ‰í•˜ì§€ ëª»í•¨

## ğŸ“‰ v05 ëŒ€ë¹„ ë¹„êµ

| ì§€í‘œ | v05 (Baseline) | v10 (RL) | ì°¨ì´ |
|------|----------------|----------|------|
| **ì „ëµ** | EMA Cross (Rule) | PPO (RL) | - |
| **2024 ìˆ˜ìµë¥ ** | 293.38% | 0.00% | **-293.38%p** |
| **2025 ìˆ˜ìµë¥ ** | 124.93% | 0.00% | **-124.93%p** |
| **2024 ê±°ë˜** | 4íšŒ | 0íšŒ | -4íšŒ |
| **2025 ê±°ë˜** | 5íšŒ | 0íšŒ | -5íšŒ |
| **Sharpe** | 1.76 | N/A | - |
| **MDD** | 29.10% | 0.00% | ë§¤ìˆ˜ ì—†ì–´ì„œ ì†ì‹¤ë„ ì—†ìŒ |
| **ìŠ¹ë¥ ** | 50-60% | N/A | - |

## ğŸ’¡ í•µì‹¬ êµí›ˆ

### 1. RL â‰  ë§ŒëŠ¥ í•´ê²°ì±…

- ê·œì¹™ ê¸°ë°˜ ì „ëµ (v05)ì´ RL (v10)ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ìš°ìˆ˜
- RLì€ Reward ì„¤ê³„, State ì„¤ê³„, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ëª¨ë‘ ì™„ë²½í•´ì•¼ í•¨
- ì˜ëª»ëœ ì„¤ê³„ â†’ ì™„ì „ ì‹¤íŒ¨

### 2. Reward Shapingì˜ ì¤‘ìš”ì„±

**ì‹¤íŒ¨í•œ Reward**:
```python
reward = profit - fee - drawdown - holding_penalty
```

**ê°œì„  ë°©í–¥**:
```python
# ë°©ë²• 1: ê¸°ì¤€ì„  ëŒ€ë¹„ ì„±ê³¼
reward = (current_equity - buyhold_equity) / initial_capital

# ë°©ë²• 2: Sharpe Ratio ìµœëŒ€í™”
reward = (returns.mean() / returns.std()) * sqrt(252)

# ë°©ë²• 3: ì •ê¸°ì  Reward
reward = daily_return + long_term_profit_bonus
```

### 3. Sparse Reward ë¬¸ì œ

**í˜„ì¬**:
- ê±°ë˜ ì™„ë£Œ ì‹œì—ë§Œ Reward (Sparse)
- Episode ê¸¸ì´ 1,880 â†’ Reward ì‹ í˜¸ ë„ˆë¬´ ë“œë¬¸ë“œë¬¸

**í•´ê²°ì±…**:
- Dense Reward: ë§¤ ìŠ¤í…ë§ˆë‹¤ Equity ë³€í™”ì— ëŒ€í•œ Reward
- Intermediate Reward: ì¼ì • ì´ìµ/ì†ì‹¤ ë‹¬ì„± ì‹œ ë³´ë„ˆìŠ¤

### 4. Baseline Policy ë¶€ì¬

**ë¬¸ì œ**:
- PPOê°€ Random Policyì—ì„œ ì‹œì‘
- ì¢‹ì€ ì „ëµì„ ë°œê²¬í•˜ê¸°ê¹Œì§€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼

**í•´ê²°ì±…**:
- Behavioral Cloning: v05 ì „ëµì„ Imitation Learningìœ¼ë¡œ ì´ˆê¸°í™”
- Warm Start: v05 ê·œì¹™ì„ ì´ˆê¸° ì •ì±…ìœ¼ë¡œ ì‚¬ìš©

### 5. í™˜ê²½ ê²€ì¦ ë¶€ì¡±

**ì‹¤ìˆ˜**:
- í™˜ê²½ë§Œ í…ŒìŠ¤íŠ¸í•˜ê³  Random Agent ê²€ì¦ ìƒëµ
- Random Agentë§Œ ëŒë ¤ë´ë„ ë¬¸ì œë¥¼ ì¡°ê¸° ë°œê²¬ ê°€ëŠ¥

**êµí›ˆ**:
- í•™ìŠµ ì „ Random Agentë¡œ í™˜ê²½ ë™ì‘ í™•ì¸
- Reward ë¶„í¬, Action ë¶„í¬, Episode Length ê²€ì¦

## ğŸ”§ ê°œì„  ë°©ì•ˆ

### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ 

#### 1. Reward Function ì¬ì„¤ê³„
```python
def calculate_reward(self, prev_equity, current_equity, action):
    # 1. Equity ë³€í™”ìœ¨ (ë§¤ ìŠ¤í…)
    equity_change = (current_equity - prev_equity) / prev_equity
    reward = equity_change * 100  # 0.01 â†’ 1.0

    # 2. Buy&Hold ëŒ€ë¹„ ì´ˆê³¼ ìˆ˜ìµ
    buyhold_equity = self.initial_capital * (current_price / self.start_price)
    excess_return = (current_equity - buyhold_equity) / buyhold_equity
    reward += excess_return * 10

    # 3. ê±°ë˜ ë³´ë„ˆìŠ¤ (ìµœì†Œ í™œë™ ë³´ì¥)
    if abs(action) > 0.1:
        reward += 0.1  # ê±°ë˜ ì‹œë„ì— ì†ŒëŸ‰ ë³´ìƒ

    # 4. ì¥ê¸° ë³´ìœ  ë³´ë„ˆìŠ¤
    if self.position and self.profit > 0.10:
        reward += 1.0  # 10% ì´ìƒ ìˆ˜ìµ ìœ ì§€ ì‹œ ë³´ë„ˆìŠ¤

    return reward
```

#### 2. Curriculum Learning
```python
# Phase 1: ë‹¨ìˆœ í™˜ê²½ (100 ìº”ë“¤)
train_phase1(df[:100])

# Phase 2: ì¤‘ê°„ í™˜ê²½ (500 ìº”ë“¤)
train_phase2(df[:500])

# Phase 3: ì „ì²´ í™˜ê²½ (1,880 ìº”ë“¤)
train_phase3(df)
```

#### 3. Exploration Bonus
```python
# PPO ì„¤ì •
model = PPO(
    policy="MlpPolicy",
    env=env,
    ent_coef=0.01,  # 0.0 â†’ 0.01 (íƒí—˜ ì¥ë ¤)
    exploration_fraction=0.5,  # í•™ìŠµ ì´ˆë°˜ 50%ëŠ” íƒí—˜
    ...
)
```

#### 4. Behavioral Cloning (v05 ì „ëµ ëª¨ë°©)
```python
# v05 ì „ëµìœ¼ë¡œ Expert Trajectory ìƒì„±
expert_actions = run_v05_on_training_data()

# BCë¡œ ì´ˆê¸° ì •ì±… í•™ìŠµ
bc_model.train(expert_actions)

# BC ì •ì±…ìœ¼ë¡œ PPO ì´ˆê¸°í™”
ppo_model.load_policy(bc_model)
ppo_model.learn(total_timesteps=100_000)
```

### ì¥ê¸°ì  ê°œì„  ë°©í–¥

#### 1. ë©€í‹° Agent ì•™ìƒë¸”
- Conservative Agent (DQN)
- Balanced Agent (PPO)
- Aggressive Agent (SAC)
- ì‹œì¥ ìƒí™©ë³„ Agent ì„ íƒ

#### 2. Meta-Learning
- MAML (Model-Agnostic Meta-Learning)
- ë‹¤ì–‘í•œ ì‹œì¥ ì¡°ê±´ì—ì„œ ë¹ ë¥´ê²Œ ì ì‘

#### 3. Hierarchical RL
- High-Level: ì‹œì¥ ìƒí™© ë¶„ë¥˜ (Bull/Bear/Sideways)
- Low-Level: ê° ìƒí™©ë³„ ê±°ë˜ ì „ëµ

#### 4. Transformer-based Policy
- Attention Mechanismìœ¼ë¡œ ì¥ê¸° ì˜ì¡´ì„± í•™ìŠµ
- Time-series Transformer

## ğŸ¯ ë‹¤ìŒ ë²„ì „ ì œì•ˆ: v11

### v11A: Reward Redesign (ë¹ ë¥¸ ê²€ì¦)
- Reward Function ì¬ì„¤ê³„
- Behavioral Cloningìœ¼ë¡œ v05 ëª¨ë°©
- Timesteps 50ë§Œ â†’ 100ë§Œ
- **ì˜ˆìƒ ê¸°ê°„**: 2-3ì¼
- **ì˜ˆìƒ ì„±ê³¼**: 50-100% (v05 ëŒ€ë¹„ ì—¬ì „íˆ ë‚®ìŒ)

### v11B: Rule-based + RL Hybrid (ë³´ìˆ˜ì )
- v05 ì „ëµì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
- RL AgentëŠ” Position Sizingë§Œ ê²°ì •
- Entry/ExitëŠ” v05 ê·œì¹™ ìœ ì§€
- **ì˜ˆìƒ ê¸°ê°„**: 1-2ì¼
- **ì˜ˆìƒ ì„±ê³¼**: 100-150% (v05ì™€ ìœ ì‚¬)

### v11C: v05 ìµœì í™” (í˜„ì‹¤ì )
- RL í¬ê¸°, v05 ê°œì„ ì— ì§‘ì¤‘
- Optunaë¡œ íŒŒë¼ë¯¸í„° ì •ë°€ íŠœë‹
- Multi-Entry Conditions ì¶”ê°€
- Dynamic Trailing Stop
- **ì˜ˆìƒ ê¸°ê°„**: 1ì¼
- **ì˜ˆìƒ ì„±ê³¼**: 150-200% (v05 ëŒ€ë¹„ +50%p)

## ê¶Œì¥ ì‚¬í•­

### âœ… ì¦‰ì‹œ ì‹¤í–‰: v11C (v05 ìµœì í™”)
**ì´ìœ **:
1. RLì€ ì‹œê°„ ëŒ€ë¹„ ì„±ê³¼ê°€ ë¶ˆí™•ì‹¤
2. v05ëŠ” ì´ë¯¸ ê²€ì¦ëœ ìš°ìˆ˜í•œ ì „ëµ
3. ì‘ì€ ê°œì„ ìœ¼ë¡œë„ í° íš¨ê³¼ ê°€ëŠ¥ (293% â†’ 350%+)

**êµ¬ì²´ì  ê³„íš**:
1. Optunaë¡œ v05 íŒŒë¼ë¯¸í„° ì¬ìµœì í™” (trailing_stop 19-23% íƒìƒ‰)
2. Multi-Entry Conditions (EMA + RSI + Breakout + Momentum)
3. Adaptive Trailing Stop (ë³€ë™ì„± ê¸°ë°˜ 15-25%)
4. Walk-Forward Validation (2024ë…„ 12ê°œì›”)

**ì˜ˆìƒ ê²°ê³¼**:
- 2024: 150-180% (ëª©í‘œ 170%)
- 2025: 40-60% (ëª©í‘œ 50%)
- ì˜¤ë²„í”¼íŒ…: <50%

### âš ï¸ ì¥ê¸° ê³¼ì œ: RL ì¬ë„ì „ (v12+)
**ì¡°ê±´**:
- v11C ì„±ê³µ í›„
- Reward Function ì¬ì„¤ê³„ ì™„ë£Œ
- Baseline Policy (v11C) í™•ë³´
- ì¶©ë¶„í•œ ì‹œê°„ íˆ¬ì ê°€ëŠ¥ (1ì£¼+)

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
strategies/v10_rl_hybrid/
â”œâ”€â”€ trading_env.py              # Gym í™˜ê²½ (ë¬¸ì œ: Reward ì„¤ê³„ ì˜¤ë¥˜)
â”œâ”€â”€ train_ppo.py                # PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ì‹¤í–‰ ì™„ë£Œ)
â”œâ”€â”€ train_dqn.py                # DQN ìŠ¤í¬ë¦½íŠ¸ (ë¯¸ì‹¤í–‰, Action Space ë¶ˆì¼ì¹˜)
â”œâ”€â”€ ppo_results.json            # ê²°ê³¼ (0% ìˆ˜ìµë¥ )
â”œâ”€â”€ train_ppo.log               # í•™ìŠµ ë¡œê·¸
â”œâ”€â”€ models/ppo_balanced/        # í•™ìŠµëœ ëª¨ë¸
â”‚   â”œâ”€â”€ ppo_final.zip
â”‚   â”œâ”€â”€ best_model.zip
â”‚   â””â”€â”€ ppo_checkpoint_*.zip
â”œâ”€â”€ logs/                       # Tensorboard ë¡œê·¸
â”œâ”€â”€ requirements.txt            # ì˜ì¡´ì„±
â””â”€â”€ result.md                   # ë³¸ ë¬¸ì„œ
```

## ğŸ“Š ìµœì¢… í‰ê°€

| ì§€í‘œ | ëª©í‘œ | ë‹¬ì„± | í‰ê°€ |
|------|------|------|------|
| 2024 ìˆ˜ìµë¥  >= v05 | âœ… | âŒ 0% (vs v05 293%) | **FAIL** |
| 2025 ìˆ˜ìµë¥  >= v05 | âœ… | âŒ 0% (vs v05 125%) | **FAIL** |
| ì˜¤ë²„í”¼íŒ… < 50% | âœ… | N/A | **N/A** |
| ê±°ë˜ ë¹ˆë„ 15-25/ë…„ | âœ… | âŒ 0íšŒ/ë…„ | **FAIL** |
| Sharpe >= 2.0 | âœ… | N/A | **N/A** |

**ì¢…í•© í‰ê°€**: **F (ì‹¤íŒ¨)** ğŸ˜

---

**ì‘ì„±ì¼**: 2025-10-19
**ì‘ì„±ì**: Claude (v10 Developer)
**ë²„ì „**: v10 Final Report
**ë‹¤ìŒ ë‹¨ê³„**: v11C (v05 ìµœì í™”) ê¶Œì¥
